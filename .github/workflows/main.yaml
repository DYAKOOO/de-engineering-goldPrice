name: Gold Price Data Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * *' # Runs at 00:00 UTC every day

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: us-west1

jobs:
  deploy:
    name: Deploy to GCP
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    - name: Setup Go
      uses: actions/setup-go@v2
      with:
        go-version: '1.21'

    - name: Setup Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create Service Account Key File
      run: |
        echo '${{ secrets.GCP_SA_KEY }}' > sa-key.json
        chmod 600 sa-key.json

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ env.PROJECT_ID }}

    - name: Configure Docker
      run: |
        gcloud auth configure-docker gcr.io

    - name: Get latest version and increment
      id: get_version
      run: |
        LATEST_VERSION=$(gcloud container images list-tags gcr.io/${{ env.PROJECT_ID }}/gold-price-producer --format='get(tags)' --sort-by=~tags | grep '^v1\.0\.' | head -n 1)
        PATCH_VERSION=$(echo $LATEST_VERSION | cut -d. -f3)
        NEW_PATCH_VERSION=$((PATCH_VERSION + 1))
        NEW_VERSION="v1.0.$NEW_PATCH_VERSION"
        echo "NEW_VERSION=$NEW_VERSION" >> $GITHUB_OUTPUT

    - name: Build and Push New Version
      env:
        NEW_VERSION: ${{ steps.get_version.outputs.NEW_VERSION }}
      run: |
        docker build -t gcr.io/${{ env.PROJECT_ID }}/gold-price-producer:$NEW_VERSION -f Dockerfile-producer .
        docker push gcr.io/${{ env.PROJECT_ID }}/gold-price-producer:$NEW_VERSION

    - name: Upload code to GCS
      run: |
        gsutil cp *.py gs://de-goldprice-code/
        echo "Listing contents of GCS bucket:"
        gsutil ls gs://de-goldprice-code/

    - name: Deploy to Cloud Run
      env:
        NEW_VERSION: ${{ steps.get_version.outputs.NEW_VERSION }}
      run: |
        gcloud run deploy gold-price-ingestion \
          --image gcr.io/${{ env.PROJECT_ID }}/gold-price-producer:$NEW_VERSION \
          --region ${{ env.REGION }} \
          --set-env-vars GCS_BUCKET=gold-price-raw-data \
          --set-env-vars GOLD_API_KEY="${{ secrets.GOLD_API_KEY }}" \
          --set-env-vars GOLD_API_BASE_URL=https://www.goldapi.io/api \
          --set-env-vars GOOGLE_CLOUD_PROJECT=${{ env.PROJECT_ID }} \
          --set-env-vars PUBSUB_TOPIC=gold-price \
          --service-account goldprice-service-account@${{ env.PROJECT_ID }}.iam.gserviceaccount.com

    - name: Update DAG file
      run: |
        sed -i 's/from data_sources/from de_goldprice.data_sources/' dags/gold_price_dag.py
        sed -i 's/from pubsub_producer/from de_goldprice.pubsub_producer/' dags/gold_price_dag.py
        gsutil cp dags/gold_price_dag.py gs://us-west1-gold-price-compose-71bd680f-bucket/dags/

    - name: Check Composer Environment Status
      id: check_composer
      run: |
        MAX_RETRIES=10
        RETRY_INTERVAL=60
        for i in $(seq 1 $MAX_RETRIES); do
          STATUS=$(gcloud composer environments describe gold-price-composer --location ${{ env.REGION }} --format="value(state)")
          if [ "$STATUS" = "RUNNING" ]; then
            echo "Composer environment is running."
            echo "composer_ready=true" >> $GITHUB_OUTPUT
            break
          elif [ "$STATUS" = "UPDATING" ]; then
            echo "Composer environment is updating. Waiting..."
            sleep $RETRY_INTERVAL
          else
            echo "Composer environment is in an unexpected state: $STATUS"
            echo "composer_ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        done
        if [ "$STATUS" != "RUNNING" ]; then
          echo "Composer environment did not become ready in time."
          echo "composer_ready=false" >> $GITHUB_OUTPUT
          exit 1
        fi

    - name: Upload SSH key to Composer
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        echo "${{ secrets.SPARK_SSH_PRIVATE_KEY }}" | gcloud composer environments storage data import \
          --environment gold-price-composer \
          --location ${{ env.REGION }} \
          --destination spark_ssh_key

    - name: Configure SSH connection
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        SPARK_IP=$(gcloud compute instances describe spark-instance --zone ${{ env.REGION }}-a --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
        gcloud composer environments run gold-price-composer \
          --location ${{ env.REGION }} \
          connections -- delete 'spark_instance_ssh_hook'
        gcloud composer environments run gold-price-composer \
          --location ${{ env.REGION }} \
          connections -- add 'spark_instance_ssh_hook' \
          --conn-type 'ssh' \
          --conn-host "$SPARK_IP" \
          --conn-login 'ubuntu' \
          --conn-port 22 \
          --conn-extra '{"key_file": "/home/airflow/gcs/data/spark_ssh_key"}'

    - name: Verify SSH key in Composer
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        gcloud composer environments storage data list \
          --environment gold-price-composer \
          --location ${{ env.REGION }} \
          | grep spark_ssh_key

    - name: Test SSH Connection from Composer
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        gcloud composer environments run gold-price-composer \
          --location ${{ env.REGION }} \
          python -- -c "
        from airflow.providers.ssh.hooks.ssh import SSHHook
        hook = SSHHook(ssh_conn_id='spark_instance_ssh_hook')
            stdin, stdout, stderr = ssh_client.exec_command('echo SSH connection successful')
            print(stdout.read().decode())"
    
    - name: Configure Firewall for Spark Instance
      run: |
        if ! gcloud compute firewall-rules describe allow-composer-to-spark &>/dev/null; then
          gcloud compute firewall-rules create allow-composer-to-spark \
            --direction=INGRESS \
            --priority=1000 \
            --network=default \
            --action=ALLOW \
            --rules=tcp:22 \
            --source-ranges=$(gcloud composer environments describe gold-price-composer --location ${{ env.REGION }} --format="get(config.nodeConfig.composerInternalIpv4CidrBlock)")
        else
          echo "Firewall rule allow-composer-to-spark already exists."
        fi
    
    - name: Upload Python scripts to Composer
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        gsutil cp data_sources.py pubsub_producer.py gs://us-west1-gold-price-compose-71bd680f-bucket/dags/

    - name: Upload DAG to Cloud Composer
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        gcloud composer environments storage dags import \
          --environment gold-price-composer \
          --location ${{ env.REGION }} \
          --source dags/gold_price_dag.py

    - name: Trigger Airflow DAG
      if: steps.check_composer.outputs.composer_ready == 'true'
      run: |
        if ! gcloud composer environments run gold-price-composer \
          --location ${{ env.REGION }} \
          dags trigger -- gold_price_pipeline; then
          echo "Failed to trigger DAG. Check Airflow logs for more details."
          exit 1
        fi

    - name: Verify BigQuery Data
      run: |
        ROW_COUNT=$(bq query --use_legacy_sql=false --format=csv '
        SELECT COUNT(*) as row_count
        FROM `de-goldprice.gold_price_dataset.gold_prices`
        WHERE DATE(date) = DATE(CURRENT_TIMESTAMP())
        ' | tail -n 1)
        if [ "$ROW_COUNT" -eq "0" ]; then
          echo "No data found for today in BigQuery. This may indicate an issue with the pipeline."
          exit 1
        else
          echo "Found $ROW_COUNT rows for today in BigQuery."
        fi

    - name: Clean up old images
      run: |
        OLD_VERSIONS=$(gcloud container images list-tags gcr.io/${{ env.PROJECT_ID }}/gold-price-producer --format='get(tags)' --sort-by=~tags | tail -n +6)
        for version in $OLD_VERSIONS; do
          gcloud container images delete gcr.io/${{ env.PROJECT_ID }}/gold-price-producer:$version --quiet
        done