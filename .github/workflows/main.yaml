name: Gold Price Data Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * *' # Runs at 00:00 UTC every day

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  IMAGE_PRODUCER: gold-price-producer
  IMAGE_TAG: ${{ github.sha }}
  REGION: us-west1

jobs:
  deploy:
    name: Deploy to GCP
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - name: Setup Go
      uses: actions/setup-go@v2
      with:
        go-version: '1.21'

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ env.PROJECT_ID }}

    - name: Configure Docker
      run: |
        gcloud auth configure-docker ${REGION}-docker.pkg.dev

    - name: Create Artifact Registry Repository
      run: |
        if ! gcloud artifacts repositories describe gold-price-repo \
          --location=${REGION} \
          --format="value(name)" 2>/dev/null; then
          gcloud artifacts repositories create gold-price-repo \
            --repository-format=docker \
            --location=${REGION} \
            --description="Gold Price Analysis Docker Repository"
        fi

    - name: Build and Push Producer Image
      run: |
        docker build -t ${REGION}-docker.pkg.dev/${PROJECT_ID}/gold-price-repo/${IMAGE_PRODUCER}:${IMAGE_TAG} -f Dockerfile-producer .
        docker push ${REGION}-docker.pkg.dev/${PROJECT_ID}/gold-price-repo/${IMAGE_PRODUCER}:${IMAGE_TAG}

    - name: Install Pulumi CLI
      uses: pulumi/setup-pulumi@v2

    - name: Deploy Infrastructure with Pulumi
      run: |
        cd infrastructure
        go mod download
        pulumi stack select dev
        pulumi config set gcp:project $PROJECT_ID
        pulumi up --yes
      env:
        PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

    - name: Update Cloud Run Service
      run: |
        gcloud run services update gold-price-ingestion \
          --image ${REGION}-docker.pkg.dev/${PROJECT_ID}/gold-price-repo/${IMAGE_PRODUCER}:${IMAGE_TAG} \
          --region ${REGION}

    - name: Update dependencies
      run: |
        pip install -r requirements.txt --upgrade

    - name: Deploy Cloud Function
      run: |
        gcloud functions deploy process_gold_price \
          --gen2 \
          --runtime=python310 \
          --source=. \
          --region=${REGION} \
          --entry-point=process_pubsub \
          --trigger-http \
          --allow-unauthenticated

    - name: Trigger Cloud Run Service
      run: |
        CLOUD_RUN_URL=$(gcloud run services describe gold-price-ingestion --region ${REGION} --format='value(status.url)')
        curl -X GET ${CLOUD_RUN_URL}/fetch-and-publish

    - name: Setup Spark Instance
      run: |
        gcloud compute ssh spark-instance --zone us-west1-a --command "
        sudo apt-get update
        sudo apt-get install -y python3-pip openjdk-11-jdk
        pip3 install pyspark google-cloud-storage google-cloud-bigquery
        wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar -P /home/runner/
        "

    - name: Copy Spark scripts to Compute Engine
      run: |
        gcloud compute scp spark_jobs/clean_transform.py spark-instance:~/clean_transform.py --zone us-west1-a
        gcloud compute scp spark_jobs/load_to_bigquery.py spark-instance:~/load_to_bigquery.py --zone us-west1-a

    - name: Run Spark Jobs
      run: |
        gcloud compute ssh spark-instance --zone us-west1-a --command "
        export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json
        spark-submit \
        --master local[*] \
        --jars /home/runner/gcs-connector-hadoop3-latest.jar \
        --conf spark.hadoop.google.cloud.auth.service.account.enable=true \
        --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/your/service-account-key.json \
        --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \
        --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \
        clean_transform.py && \
        spark-submit \
        --master local[*] \
        --jars /home/runner/gcs-connector-hadoop3-latest.jar \
        --conf spark.hadoop.google.cloud.auth.service.account.enable=true \
        --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/your/service-account-key.json \
        --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \
        --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \
        load_to_bigquery.py
        "
    - name: Verify Deployment
      run: |
        gcloud run services describe gold-price-ingestion --region ${REGION}
        gcloud functions describe process_gold_price --region ${REGION}
        gcloud compute instances describe spark-instance --zone us-west1-a